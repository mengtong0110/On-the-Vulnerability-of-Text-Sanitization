import os
import json
from nltk.tokenize import word_tokenize
import tqdm
import random
random.seed(42)
from task_set import task,eps_list,init_santext,init_eps
from SanText import SanText_plus

with open('sen_words.txt','r') as sen_file:
    sen_words=json.load(sen_file)   

shallow_percentage=0.05

task_num=20000

def load_sentence(data_path1,categories_s):
    with open(data_path1, 'r', encoding="utf8") as file2:
            data2=json.load(file2)
    with open(data_path1, 'r', encoding="utf8") as file:
            data_=json.load(file)
    test_data = []

    for san_token in tqdm.tqdm(categories_s,desc="loadnig data"):     
            for sample in data_:
                for i in range(len(sample[0])):
                    if sample[1][i]==san_token and sample[0][i] in sen_words:
                                test_data.append(sample[0])
        
    random.shuffle(test_data)
    test_data=test_data[:1000]
    return test_data


if task=="sst2":
        Data_path="./Dataset/SST-2/train.tsv"
        with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)
        init_santext(task)
        for eps in eps_list: 
                init_eps(eps)
                data_path = f"./data/private_datasets/{eps}_{task}.txt"
                with open(f'./data/shadow_datasets/theorem_{eps}_sst2.txt','w') as P_file:            
                        json_file=[]
                        raw_sent=load_sentence(data_path,categories_s)
                        for i in tqdm.trange(len(raw_sent)):
                                json_temp=[]
                                words = raw_sent[i]
                                json_temp.append(words)
                                for k in range(30):
                                        new_first_50_words=SanText_plus(words)
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4)  

if task=="qnli":
        Data_path="./Dataset/QNLI/train.tsv"
        with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)
        init_santext(task)
        for eps in eps_list: 
                init_eps(eps)
                data_path = f"./data/private_datasets/{eps}_{task}.txt"
                with open(f'./data/shadow_datasets/theorem_{eps}_qnli.txt','w') as P_file:            
                        json_file=[]
                        raw_sent=load_sentence(data_path,categories_s)
                        for i in tqdm.trange(len(raw_sent)):
                                json_temp=[]
                                words = raw_sent[i]
                                json_temp.append(words)
                                for k in range(30):
                                        new_first_50_words=SanText_plus(words)
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4)  

if task=="agnews":
        Data_path="./Dataset/AGNEWS/train.tsv"
        with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)
        init_santext(task)
        for eps in eps_list: 
                init_eps(eps)
                data_path = f"./data/private_datasets/{eps}_{task}.txt"
                with open(f'./data/shadow_datasets/theorem_{eps}_agnews.txt','w') as P_file:            
                        json_file=[]
                        raw_sent=load_sentence(data_path,categories_s)
                        for i in tqdm.trange(len(raw_sent)):
                                json_temp=[]
                                words = raw_sent[i]
                                json_temp.append(words)
                                for k in range(30):
                                        new_first_50_words=SanText_plus(words)
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)
                        json.dump(json_file, P_file, indent=4)                                                        

print(f"\n=====Sanitized sentences for bounds on {task} are already in folders.=====\n")



