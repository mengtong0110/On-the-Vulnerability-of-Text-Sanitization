
import os
import json
from SanText import SanText_plus
from nltk.tokenize import word_tokenize
import tqdm
import random
random.seed(42)
from task_set import task,eps_list,init_santext_first,init_eps

def Data_loader_sst2(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        next(file)
        all_data=[]  
        for row in file:
                sentence,label=row.strip().split('\t')
                all_data.append(sentence)
        random.shuffle(all_data)
        return all_data[:task_num]

def Data_loader_qnli(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        next(file)
        all_data=[]  
        for row in file:
                index,question,sentence,label=row.strip().split('\t')
                all_data.append(sentence)
        random.shuffle(all_data)
        return all_data[:task_num]
def Data_loader_agnews(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        all_data=[]  
        for row in file:
                sentence=row.strip()
                all_data.append(sentence)
        random.shuffle(all_data)
        return all_data[:task_num]        


task_num=20000

if task=="sst2":
        Data_path="./Dataset/SST-2/train.tsv"
        raw_data=Data_loader_sst2(Data_path)
        init_santext_first(task) 
        for eps in eps_list:
                init_eps(eps) 
                with open(f'./data/private_datasets/{eps}_sst2.txt','w') as P_file:              
                        json_file=[]
                        for i in tqdm.trange(int(task_num)):
                                json_temp=[]
                                data=raw_data[i]
                                words = word_tokenize(data)
                                first_50_words = words
                                json_temp.append(first_50_words)
                                for k in range(1):
                                        new_first_50_words=SanText_plus(first_50_words )
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4) 

if task=="qnli":
        Data_path="./Dataset/QNLI/train.tsv"
        raw_data=Data_loader_qnli(Data_path)
        init_santext_first(task)
        for eps in eps_list:
                init_eps(eps)           
                with open(f'./data/private_datasets/{eps}_qnli.txt','w') as P_file:           
                        json_file=[]
                        for i in tqdm.trange(int(task_num)):
                                json_temp=[]
                                data=raw_data[i]
                                words = word_tokenize(data)
                                first_50_words = words
                                json_temp.append(first_50_words)
                                for k in range(1):
                                        new_first_50_words=SanText_plus(first_50_words)
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)
                        json.dump(json_file, P_file, indent=4) 
                                                                
if task=="agnews":
        Data_path="./Dataset/AGNEWS/train.tsv"
        raw_data=Data_loader_agnews(Data_path)
        init_santext_first(task)
        for eps in eps_list:
                init_eps(eps) 
                with open(f'./data/private_datasets/{eps}_agnews.txt','w') as P_file:         
                        json_file=[]
                        for i in tqdm.trange(int(task_num)):
                                json_temp=[]
                                data=raw_data[i]
                                words = word_tokenize(data)
                                first_50_words = words
                                json_temp.append(first_50_words)
                                for k in range(1):
                                        new_first_50_words=SanText_plus(first_50_words)
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)
                        json.dump(json_file, P_file, indent=4) 
print(f"\n=====Sanitized sentences of the private dataset on {task} are already in folders.=====\n")



