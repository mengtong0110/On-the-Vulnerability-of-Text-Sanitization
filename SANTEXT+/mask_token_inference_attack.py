import torch
from transformers import BertTokenizer, BertForMaskedLM
import json
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset, SequentialSampler
import copy
import os
import string
from task_set import task,eps_list
import random
random.seed(42)
torch.manual_seed(42)
model_path = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForMaskedLM.from_pretrained(model_path)
model.eval()
with open('sen_words.txt','r') as sen_file:
    sen_words=json.load(sen_file) 

def load_sentence(data_path1,data_path2, categories_s):

    test_data = []
    train_data = []
    for san_token in  tqdm(categories_s):           
        with open(data_path1, 'r', encoding="utf8") as file:
            data_=json.load(file)
            for sample in data_:
                for i in range(len(sample[0])):
                    if sample[1][i]==san_token and sample[0][i] in sen_words:
                        BY_c=[]
                        BY_t2=[]
                        BY_t1=[]
                        if(R_dict[sample[1][i]][0]==sample[0][i]):
                            label=1
                        else:
                            label=0
                        for j in range(len(sample[0])):
                            
                            if(j<i):
                                BY_c.append(sample[0][j])
                                BY_t1.append(sample[1][j])
                            
                            elif(j>i):
                                BY_c.append(sample[0][j])
                                BY_t1.append(sample[1][j])
                            else:
                                BY_c.append(sample[0][j])
                                BY_t1.append(sample[1][j])
                        test_data.append([BY_c,BY_t1,BY_t2,sample[0][i],sample[1][i],i])


        with open(data_path2, 'r', encoding="utf8") as file2:
            data2=json.load(file2)
            for sample2 in data2:
                for i in range(len(sample2[0])):
                    for k in range(1,101):
                        if sample2[k][i]==san_token and sample2[0][i] in sen_words:
                                BY_c=[]
                                BY_t=[]
                                if(R_dict[sample2[k][i]][0]==sample2[0][i]):
                                    label=1
                                else:
                                    label=0
                                for j in range(len(sample2[0])):

                                    if(j!=i):
                                        if sample2[k][j] in R_dict and sample2[0][j] in sen_words:
                                            BY_c.append(sample2[k][j])
                                            BY_t.append(sample2[k][j])
                                        else:
                                            BY_c.append(sample2[k][j])
                                            BY_t.append(sample2[k][j])
                                    else:
                                        BY_c.append(sample2[k][j])
                                        BY_t.append(R_dict[sample2[k][i]][0])
                                BY_c=" ".join(BY_c)
                                BY_t=" ".join(BY_t)
                                train_data.append([BY_c,BY_t,label])
    random.shuffle(test_data)
    test_data=test_data[:1000]                            
    return train_data,test_data
   

def mask_token_inference_attack_bert(tokenized_raw_text, tokenized_sanitized_text,idx):


    tokenized_new_docs = []
    labels = []
    unk_id = tokenizer.convert_tokens_to_ids("[UNK]")  

    for i, new_doc in enumerate(tokenized_sanitized_text):
        assert len(tokenized_raw_text[i]) == len(new_doc)
        raw_token_id = tokenizer.convert_tokens_to_ids(tokenized_raw_text[i][idx])
        if raw_token_id == unk_id:  
            continue

        tmp_doc = copy.deepcopy(new_doc)
        tmp_doc[idx] = "[MASK]"
        tokenized_new_docs.append(tokenizer.encode_plus(tmp_doc, padding="max_length", max_length=64, truncation=True))
        labels.append(raw_token_id)

    all_input_ids = torch.tensor([doc['input_ids'] for doc in tokenized_new_docs], dtype=torch.long)
    all_attention_mask = torch.tensor([doc['attention_mask'] for doc in tokenized_new_docs], dtype=torch.long)
    all_labels = torch.tensor(labels, dtype=torch.long)
    dataset = TensorDataset(all_input_ids, all_attention_mask, all_labels)
    dataloader = DataLoader(dataset, batch_size=256, sampler=SequentialSampler(dataset))

    intersect_num = 0
    total_num = 0
    for batch in dataloader:
        with torch.no_grad():
            inputs = {
                "input_ids": batch[0],
                "attention_mask": batch[1]
            }
            prediction = model(**inputs)[0]

            topk_predictions = torch.topk(prediction, 1, dim=2)[1]

            mask_positions = torch.where(batch[0] == tokenizer.convert_tokens_to_ids("[MASK]"))
            ground_truths = batch[2]

            match_mask = torch.any((topk_predictions[mask_positions] == ground_truths.unsqueeze(1)), dim=1)

            intersect_num += match_mask.sum().item()
            total_num

    return intersect_num 

for eps in eps_list:
    with open(f'./data/attack_result/Mask_attack/{eps}_{task}.txt','w') as R_file:
        total_correct_predictions = 0
        total_evaluated_samples = 0
        data_path1 = f"./data/private_datasets/{eps}_{task}.txt" 
        data_path2 = f"./data/shadow_datasets/{eps}_{task}.txt"


        with open(f"./data/reverse_dict/re_{eps}_dict.txt",'r') as Re_file:
                R_dict=json.load(Re_file)
        with open(f"./data/Scores/Bayes_score/{eps}_{task}.txt",'r') as Rep_file:
                RP_dict=json.load(Rep_file)

        with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)

        train_data, test_data= load_sentence(data_path1,data_path2,categories_s)
        for sample in tqdm(test_data,desc="Testing"):           
            raw_token=sample[3]
            
            total_correct_predictions += mask_token_inference_attack_bert([sample[0]],[sample[1]],sample[5])
            total_evaluated_samples += 1
        print(f"ASR of Mask Token Inference Attack with eps={eps} on {task} is: {total_correct_predictions/total_evaluated_samples:.4f}")
        R_file.write(f'ASR of Mask Token Inference Attack with eps={eps} on {task} is: {total_correct_predictions/total_evaluated_samples:.4f}')