from SanText import SanText_plus, SanText_plus_init
from utils import get_vocab_SST2, word_normalize,get_vocab_QNLI,get_vocab_AGNEWS
from scipy.special import softmax
from sklearn.metrics.pairwise import euclidean_distances
from tqdm import tqdm
import numpy  as np
import json
task='sst2'
eps_list=[1.8,2.0,2.2,2.4,2.6,2.8,3.0,3.2]
eps_list=[1.8,2.0,2.2]
sensitive_word_percentage = 0.999
def cal_probability(word_embed_1, word_embed_2, epsilon=2.0):
    distance = euclidean_distances(word_embed_1, word_embed_2)
    sim_matrix = -distance
    prob_matrix = softmax(epsilon * sim_matrix / 2, axis=1)
    return prob_matrix


def init_santext_first(task):
        if task == 'sst2':
            data_dir = "./Dataset/SST-2/"
            vocab = get_vocab_SST2(data_dir)
        elif task == 'qnli':
            data_dir = "./Dataset/QNLI/"
            vocab = get_vocab_QNLI(data_dir)
        else:
            data_dir = "./Dataset/AGNEWS/"
            vocab = get_vocab_AGNEWS(data_dir)
        print(f'vocab size: {len(vocab)}')
        word_embedding_path = './Dataset/glove.840B.300d.txt'
        sensitive_word_count = int(sensitive_word_percentage * len(vocab))
        words = [key for key, _ in vocab.most_common()]
        sensitive_words = words[-sensitive_word_count - 1:]
        sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}
        sensitive_word_embed = []
        all_word_embed = []
        word2id = {}
        sword2id = {}
        sensitive_count = 0
        all_count = 0
        num_lines = sum(1 for _ in open(word_embedding_path))
        with open(word_embedding_path) as f:
            for row in tqdm(f, total=num_lines - 1):
                content = row.rstrip().split(' ')
                cur_word = word_normalize(content[0])
                if cur_word in vocab and cur_word not in word2id:
                    word2id[cur_word] = all_count
                    all_count += 1
                    emb = [float(i) for i in content[1:]]
                    all_word_embed.append(emb)
                    if cur_word in sensitive_words2id:
                        sword2id[cur_word] = sensitive_count
                        sensitive_count += 1
                        sensitive_word_embed.append(emb)
        with open('sen_words.txt','w') as s_file:
             json.dump(sword2id,s_file)
        all_word_embed = np.array(all_word_embed, dtype='f')
        sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')
        global all_word_embed_
        global sensitive_word_embed_
        global word2id_
        global sword2id_
        global words_
        all_word_embed_=all_word_embed
        sensitive_word_embed_=sensitive_word_embed
        word2id_=word2id
        sword2id_=sword2id
        words_=words

def init_santext(task):
        if task == 'sst2':
            data_dir = "./Dataset/SST-2/"
            vocab = get_vocab_SST2(data_dir)
        elif task == 'qnli':
            data_dir = "./Dataset/QNLI/"
            vocab = get_vocab_QNLI(data_dir)
        else:
            data_dir = "./Dataset/AGNEWS/"
            vocab = get_vocab_AGNEWS(data_dir)
        word_embedding_path = './Dataset/glove.840B.300d.txt'
        sensitive_word_count = int(sensitive_word_percentage * len(vocab))
        words = [key for key, _ in vocab.most_common()]
        sensitive_words = words[-sensitive_word_count - 1:]
        sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}
        sensitive_word_embed = []
        all_word_embed = []
        word2id = {}
        sword2id = {}
        sensitive_count = 0
        all_count = 0
        num_lines = sum(1 for _ in open(word_embedding_path))
        with open(word_embedding_path) as f:
            for row in tqdm(f, total=num_lines - 1):
                content = row.rstrip().split(' ')
                cur_word = word_normalize(content[0])
                if cur_word in vocab and cur_word not in word2id:
                    word2id[cur_word] = all_count
                    all_count += 1
                    emb = [float(i) for i in content[1:]]
                    all_word_embed.append(emb)
                    if cur_word in sensitive_words2id:
                        sword2id[cur_word] = sensitive_count
                        sensitive_count += 1
                        sensitive_word_embed.append(emb)
        all_word_embed = np.array(all_word_embed, dtype='f')
        sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')
        global all_word_embed_
        global sensitive_word_embed_
        global word2id_
        global sword2id_
        global words_
        all_word_embed_=all_word_embed
        sensitive_word_embed_=sensitive_word_embed
        word2id_=word2id
        sword2id_=sword2id
        words_=words

def init_eps(eps):
    prob_matrix = cal_probability(all_word_embed_, sensitive_word_embed_, eps)
    SanText_plus_init(prob_matrix, word2id_, sword2id_, words_,0)        