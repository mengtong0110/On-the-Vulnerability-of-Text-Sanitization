
from data.utils import *
from data.args import *
import os
import json
from nltk.tokenize import word_tokenize
import tqdm
import random
random.seed(42)

parser = get_parser()
args = parser.parse_args()
        
stop_words = set(stopwords.words('english'))
shadow_percentage=0.05

task_num=20000
from task_set import task,eps_list
def load_sentence(data_path1,categories_s):
    with open(data_path1, 'r', encoding="utf8") as file2:
            data2=json.load(file2)
    with open(data_path1, 'r', encoding="utf8") as file:
            data_=json.load(file)
    test_data = []
    train_data = []
    for san_token in tqdm.tqdm(categories_s,desc="loadnig data"):     
            for sample in data_:
                for i in range(len(sample[0])):
                    if sample[1][i]==san_token and sample[0][i] not in stop_words and sample[0][i] in p_dict and sample[1][i] in R_dict:
                                test_data.append(sample[0])
        
    random.shuffle(test_data)
    test_data=test_data[:3000]
    return test_data


if task=="sst2":
        Data_path="./Dataset/SST-2/train.tsv"
        with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)
        for eps in eps_list:
                with open(f"./data/reverse_dict/theorem_re_{eps}_dict.txt",'r') as Re_file:
                        R_dict=json.load(Re_file) 
                parser = get_parser()
                args = parser.parse_args()
                data_path = f"./data/private_datasets/{eps}_{task}.txt"            
                with open(f'./data/shadow_datasets/theorem_{eps}_sst2.txt','w') as P_file:
                        with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                                p_dict = json.load(dic)
                        with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                                sim_word_dict = json.load(dic)                
                        json_file=[]
                        raw_sent=load_sentence(data_path,categories_s)
                        for i in tqdm.trange(len(raw_sent)):
                                json_temp=[]
                                words = raw_sent[i]
                                json_temp.append(words)
                                for k in range(30):
                                        new_first_50_words=generate_single_new_sent(old_task = words ,sim_word_dict = sim_word_dict ,p_dict = p_dict )
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4)  

if task=="qnli":
        Data_path="./Dataset/QNLI/train.tsv"
        with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)
        for eps in eps_list: 
                with open(f"./data/reverse_dict/theorem_re_{eps}_dict.txt",'r') as Re_file:
                        R_dict=json.load(Re_file) 
                parser = get_parser()
                args = parser.parse_args()
                data_path = f"./data/private_datasets/{eps}_{task}.txt"
                
                
                with open(f'./data/shadow_datasets/theorem_{eps}_qnli.txt','w') as P_file:
                        with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                                p_dict = json.load(dic)
                        with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                                sim_word_dict = json.load(dic)                
                        json_file=[]
                        raw_sent=load_sentence(data_path,categories_s)
                        for i in tqdm.trange(len(raw_sent)):
                                json_temp=[]
                                words = raw_sent[i]
                                json_temp.append(words)
                                for k in range(30):
                                        new_first_50_words=generate_single_new_sent(old_task = words ,sim_word_dict = sim_word_dict ,p_dict = p_dict )
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4) 


if task=="agnews":
        Data_path="./Dataset/AGNEWS/train.tsv"
        with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)
        for eps in eps_list: 
                with open(f"./data/reverse_dict/theorem_re_{eps}_dict.txt",'r') as Re_file:
                        R_dict=json.load(Re_file) 
                parser = get_parser()
                args = parser.parse_args()
                data_path = f"./data/private_datasets/{eps}_{task}.txt"
                
                
                with open(f'./data/shadow_datasets/theorem_{eps}_agnews.txt','w') as P_file:
                        with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                                p_dict = json.load(dic)
                        with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                                sim_word_dict = json.load(dic)                
                        json_file=[]
                        raw_sent=load_sentence(data_path,categories_s)
                        for i in tqdm.trange(len(raw_sent)):
                                json_temp=[]
                                words = raw_sent[i]
                                json_temp.append(words)
                                for k in range(30):
                                        new_first_50_words=generate_single_new_sent(old_task = words ,sim_word_dict = sim_word_dict ,p_dict = p_dict )
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4)                                                          

print(f"\n=====Sanitized sentences for ASR bounds on {task} are already in folders.=====\n")



