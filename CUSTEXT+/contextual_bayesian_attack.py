import torch
import torch.nn as nn
from nltk.corpus import stopwords
from torch.optim import Adam, lr_scheduler
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, BertTokenizer
import os
import json
import torch.nn.functional as F
from tqdm import tqdm
import random
import copy
random.seed(42)
torch.manual_seed(42)


class BertSST2Model(nn.Module):
    def __init__(self, class_size, pretrained_name='bert-base-uncased'):
        super(BertSST2Model, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name, return_dict=True)
        self.classifier = nn.Linear(768, class_size)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None):
        output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
        return self.classifier(output.pooler_output)


def load_categories(data_path1):
    categories_san = set() 
    with open(data_path1, 'r', encoding="utf8") as file:
        data_=json.load(file)
        for sample in data_:
            for i in range(len(sample[0])):
                if(sample[0][i] not in stop_words and sample[0][i] in p_dict and sample[1][i] in p_dict  and sample[1][i] in R_dict):
                    categories_san.add(sample[1][i])
    return categories_san

def load_sentence(data_path1,data_path2, categories_s):
    with open(data_path2, 'r', encoding="utf8") as file2:
            data2=json.load(file2)
    with open(data_path1, 'r', encoding="utf8") as file:
            data_=json.load(file)
    test_data = []
    train_data = []
    for san_token in tqdm(categories_s,desc="loadnig data"):     
        
            for sample in data_:
                for i in range(len(sample[0])):
                    if sample[1][i]==san_token and sample[0][i] not in stop_words and sample[0][i] in p_dict:
                        BY_c=[]
                        BY_t2=[]
                        BY_t1=[]
                        if(R_dict[sample[1][i]][0]==sample[0][i]):
                            label=1
                        else:
                            label=0
                        for j in range(len(sample[0])):
                            if len(sample[1][j])>=10:
                                continue
                            if(j<i):
                                BY_c.append(sample[1][j])
                                BY_t1.append(sample[1][j])
                            
                            elif(j>i):
                                BY_c.append(sample[1][j])
                                BY_t2.append(sample[1][j])
                            else:
                                BY_c.append(sample[1][j])

                        BY_c=" ".join(BY_c)
                        BY_t1=" ".join(BY_t1)
                        BY_t2=" ".join(BY_t2)
                        test_data.append([BY_c,BY_t1,BY_t2,sample[0][i],sample[1][i]])
        
            for sample2 in data2:
                for i in range(len(sample2[0])):
                    for k in range(1,101):
                        if sample2[k][i]==san_token:
                                if(R_dict[sample2[k][i]][0]==sample2[0][i]):
                                    BY_c=[]
                                    BY_t=[]
                                    BY_tr=[]
                                    for j in range(len(sample2[0])):
                                        
                                        if len(sample2[k][j])>=10:
                                            
                                            continue
                                        if(j!=i):
                                            BY_c.append(sample2[k][j])
                                            BY_t.append(sample2[k][j])
                                            BY_tr.append(sample2[k][j])
                                        else:
                                            BY_c.append(sample2[k][j])
                                            BY_t.append(R_dict[sample2[k][i]][0])
                                            if(len(R_dict[sample2[k][i]])>1):
                                                BY_tr.append(R_dict[sample2[k][i]][1])
                                    BY_cc=" ".join(BY_c)
                                    BY_tt=" ".join(BY_t)
                                    train_data.append([BY_cc,BY_tt,1])
                                    if(len(R_dict[sample2[k][i]])>1):
                                        BY_ttr=" ".join(BY_tr)
                                        train_data.append([BY_cc,BY_ttr,0])
                                else:
                                    
                                    BY_c=[]
                                    BY_t=[]
                                    BY_tr=[]
                                    for j in range(len(sample2[0])):
                                        
                                        if len(sample2[k][j])>=10:
                                            
                                            continue
                                        if(j!=i):
                                            BY_c.append(sample2[k][j])
                                            BY_t.append(sample2[k][j])
                                            BY_tr.append(sample2[k][j])
                                        else:
                                            BY_c.append(sample2[k][j])
                                            BY_t.append(R_dict[sample2[k][i]][0])
                                            if(len(R_dict[sample2[k][i]])>1):
                                                BY_tr.append(sample2[0][j])
                                    BY_cc=" ".join(BY_c)
                                    BY_tt=" ".join(BY_t)
                                    train_data.append([BY_cc,BY_tt,0])
                                    if(len(R_dict[sample2[k][i]])>1):
                                        BY_ttr=" ".join(BY_tr)
                                        train_data.append([BY_cc,BY_ttr,1])
    random.shuffle(train_data)
    random.shuffle(test_data)
    test_data=test_data[:3000]
    train_data=train_data
    return train_data, test_data

class BertDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        question, sentence, label = self.dataset[index]
        return question, sentence, label

def collate_fn(batch):

    questions, sentences, labels = zip(*batch)
    labels = torch.tensor([label for label in labels])
    encoding = tokenizer(list(questions), list(sentences), padding=True, truncation=True, return_tensors='pt',max_length=512)
    return encoding['input_ids'], encoding['attention_mask'], labels

stop_words = set(stopwords.words('english'))



def predict_sentiment(model, tokenizer, device, question, sentence):

    with torch.no_grad():  
        encoding = tokenizer.encode_plus(question, sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)
        
        outputs = model(input_ids, attention_mask=attention_mask)
        
        probabilities = F.softmax(outputs, dim=1)
        
        label1_probability = probabilities[0, 1].item()
        
        return label1_probability


from task_set import task,eps_list

for eps in eps_list:
    before=0
    with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
        p_dict = json.load(dic)
    total_correct_predictions = 0
    total_evaluated_samples = 0
    data_path1 = f"./data/private_datasets/{eps}_{task}.txt"  
    data_path2 = f"./data/shadow_datasets/{eps}_{task}.txt"


    with open(f"./data/reverse_dict/re_{eps}_dict.txt",'r') as Re_file:
            R_dict=json.load(Re_file)
    with open(f"./data/Scores/Bayes_score/{eps}_{task}.txt",'r') as Rep_file:
            RP_dict=json.load(Rep_file)

    with open('tokens_ctg.txt','r') as file:
            categories_s=json.load(file)

    with open(f'./data/attack_result/Bayes_attack_plus/{eps}_{task}.txt','w') as R_file:
        temp_pp=copy.deepcopy(RP_dict)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = BertSST2Model(2, 'bert-base-uncased').to(device)
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        train_data, test_data= load_sentence(data_path1,data_path2,categories_s)

        train_dataset = BertDataset(train_data)

        train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)
        optimizer = Adam(model.parameters(), lr=5e-5)
        scheduler = lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)
        loss_fn = nn.CrossEntropyLoss()


        num_epochs = 3 

        for epoch in range(1, num_epochs + 1):
            model.train()
            for input_ids, attention_mask, labels in tqdm(train_dataloader, desc=f"Training Epoch {epoch}"):
                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(input_ids, attention_mask=attention_mask)
                loss = loss_fn(outputs, labels)
                loss.backward()
                optimizer.step()
                scheduler.step()
            fall_num=0
        model.eval()
        for sample in tqdm(test_data,desc="Testing"):
            temp_p=copy.deepcopy(temp_pp)
            

            for can_token in R_dict[sample[4]]:
                raw_sen=sample[0]
                rec_sen=sample[1]+can_token+sample[2]
                raw_token=sample[3]

                Pb_token=predict_sentiment(model, tokenizer, device, raw_sen, rec_sen)
                idx=R_dict[sample[4]].index(can_token)

                temp_p[sample[4]][idx]=temp_p[sample[4]][idx]*Pb_token

            m_idx=temp_p[sample[4]].index(max(temp_p[sample[4]]))
            attack_result= R_dict[sample[4]][m_idx]
            if(raw_token in attack_result):
                total_correct_predictions += 1
            else:
                fall_num+=1
            total_evaluated_samples += 1
            before=total_correct_predictions/total_evaluated_samples
        print(f"ASR of Contextual Bayesian Attack with eps={eps} on {task} is: {total_correct_predictions/total_evaluated_samples:.4f}")
        R_file.write(f'ASR of Contextual Bayesian Attack with eps={eps} on {task} is: {total_correct_predictions/total_evaluated_samples:.4f}')

