import os
import torch
from torch.optim import Adam, lr_scheduler
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel
from tqdm import tqdm
import time
from transformers import BertTokenizer
from transformers import logging
import json
from nltk.corpus import stopwords
import random
logging.set_verbosity_error()
random.seed(42)
torch.manual_seed(42)
from task_set import task,eps_list

class BertSST2Model(nn.Module):
    def __init__(self, class_size, pretrained_name='bert-base-uncased'):
        super(BertSST2Model, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name, return_dict=True)
        self.classifier = nn.Linear(768, class_size)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None):
        output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
        return self.classifier(output.pooler_output)


def save_pretrained(model, path):
    os.makedirs(path, exist_ok=True)
    torch.save(model, os.path.join(path, 'model.pth'))

def load_categories(data_path1):
    categories_san = set() 
    with open(data_path1, 'r', encoding="utf8") as file:
        data_=json.load(file)
        for sample in data_:
            for i in range(len(sample[0])):
                if(sample[0][i] not in stop_words and sample[0][i] in p_dict):
                    categories_san.add(sample[1][i])
    return categories_san

def load_sentence_polarity(data_path1,data_path2, san_token,test_d):
    label_to_index = {label: index for index, label in enumerate(R_dict[san_token])}
    test_data = []
    train_data = []
    categories = set()     
    for sample in test_d:
        
            if sample[4]==san_token and sample[3] not in stop_words and  sample[3]  in p_dict:
              
                test_data.append((label_to_index[ sample[3] ], sample[4] ))

    with open(data_path2, 'r', encoding="utf8") as file2:
        data2=json.load(file2)
        for sample2 in data2:
            for i in range(len(sample2[0])):
                for k in range(1,101):
                    if sample2[k][i]==san_token and sample2[0][i] not in stop_words and sample2[0][i] in p_dict:
                        train_data.append((label_to_index[sample2[0][i]], sample2[k][i]))

    return train_data, test_data, categories
class BertDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        sentence, label = self.dataset[index]
        return sentence, label
def load_sentence(data_path1,categories_s):
    test_data = []
    with open(data_path1, 'r', encoding="utf8") as file:
            data_=json.load(file)
    for san_token in  tqdm(categories_s,desc=f"{task} is loading"):           
            for sample in data_:
                for i in range(len(sample[0])):
                    if sample[1][i]==san_token and sample[0][i] not in stop_words and sample[0][i] in p_dict and sample[1][i] in R_dict:
                        BY_c=[]
                        BY_t2=[]
                        BY_t1=[]
                        for j in range(len(sample[0])):
                            if len(sample[1][j])>=10:
                                continue
                            if(j<i):
                                BY_c.append(sample[1][j])
                                BY_t1.append(sample[1][j])
                            elif(j>i):
                                BY_c.append(sample[1][j])
                                BY_t2.append(sample[1][j])
                            else:
                                BY_c.append(sample[1][j])
                        BY_c=" ".join(BY_c)
                        BY_t1=" ".join(BY_t1)
                        BY_t2=" ".join(BY_t2)
                        test_data.append([BY_c,BY_t1,BY_t2,sample[0][i],sample[1][i]])
    random.shuffle(test_data)
    test_data=test_data[:3000]
    return test_data

def collate_f(batch):
    labels,sentences = zip(*batch)
    labels = torch.tensor([label for label in labels])
    encoding = tokenizer( list(sentences), padding=True, truncation=True, return_tensors='pt',max_length=512)
    return encoding['input_ids'], encoding['attention_mask'], labels
    
total_correct_predictions = 0
total_evaluated_samples = 0
stop_words = set(stopwords.words('english'))



for eps in eps_list:
    with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
        p_dict = json.load(dic)


    num_epoch = 3  
    data_path1 = f"./data/private_datasets/{eps}_{task}.txt"  
    data_path2 = f"./data/shadow_datasets/{eps}_{task}.txt"


    with open(f"./data/reverse_dict/Rever_dict.txt",'r') as Re_file:
            R_dict=json.load(Re_file)
    with open('./data/tokens_ctg.txt','r') as file:
        categories_s=json.load(file)

    al_len=0
    test_d=load_sentence(data_path1,categories_s)
    for token_s in tqdm(categories_s,desc="training"):
        label_to_index = {label: index for index, label in enumerate(R_dict[token_s])}
        train_data, test_data, categ = load_sentence_polarity(
            data_path1=data_path1,data_path2=data_path2, san_token=token_s,test_d=test_d)
        train_dataset = BertDataset(train_data)
        test_dataset = BertDataset(test_data)
        total_evaluated_samples +=len(test_dataset)
        
        if(len(train_dataset)==0 or len(test_dataset)==0):
            print(f'\n skip {len(train_dataset)} samples {len(test_dataset)}')
            continue
        
        train_dataloader = DataLoader(train_dataset,
                                    batch_size=32,
                                    collate_fn=collate_f,
                                    shuffle=True)
        test_dataloader = DataLoader(test_dataset,
                                    batch_size=32,
                                    collate_fn=collate_f)


        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


        pretrained_model_name = 'bert-base-uncased'

        model = BertSST2Model((len(R_dict[token_s])), pretrained_model_name)

        model.to(device)

        tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

        optimizer = Adam(model.parameters(), lr=5e-5)  

        timestamp = time.strftime("%m_%d_%H_%M", time.localtime())
        
        optimizer = Adam(model.parameters(), lr=5e-5)
        scheduler = lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)
        loss_fn = nn.CrossEntropyLoss()

        num_epochs = 3 

        for epoch in range(1, num_epochs + 1):
            
            model.train()
            total_train_loss = 0
            correct_predictions = 0
            total_predictions = 0
            for input_ids, attention_mask, labels in tqdm(train_dataloader, desc=f"Training Epoch {epoch}"):
                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(input_ids, attention_mask=attention_mask)
                loss = loss_fn(outputs, labels)
                loss.backward()
                optimizer.step()
                scheduler.step()

                total_train_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct_predictions += (predicted == labels).sum().item()
                total_predictions += labels.size(0)

                avg_train_loss = total_train_loss / len(train_dataloader)
                train_accuracy = correct_predictions / total_predictions
        model.eval()
        total_val_loss = 0
        correct_predictions = 0
        total_predictions = 0
        with torch.no_grad():
            for input_ids, attention_mask, labels in tqdm(test_dataloader, desc=f"Validation Epoch {epoch}"):
                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
                outputs = model(input_ids, attention_mask=attention_mask)
                _, predicted = torch.max(outputs, 1)
                correct_predictions += (predicted == labels).sum().item()
                total_predictions += labels.size(0)
       
        total_correct_predictions += correct_predictions  
    with open(f'./data/attack_result/Inv_attack/{eps}_{task}.txt','w') as R_file:
        print(f"ASR of Inversion BERT Attack with eps={eps} on {task} is:{total_correct_predictions/ total_evaluated_samples:.5f}")
        R_file.write(f'ASR of Inversion BERT Attack with eps={eps} on {task} is: {total_correct_predictions/ total_evaluated_samples:.5f}')
