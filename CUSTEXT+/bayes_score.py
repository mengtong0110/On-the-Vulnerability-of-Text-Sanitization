from data.utils import *
import transformers
transformers.logging.set_verbosity_error()
from data.args import *
import os
import json
from nltk.tokenize import word_tokenize
import tqdm
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

parser = get_parser()
args = parser.parse_args()

      
with open(f"./data/sim_word_dict/{args.embedding_type}/{args.mapping_strategy}/eps_{1.0}_top_{args.top_k}.txt", 'r') as dic:
        sim_word_dict = json.load(dic) 
re_dict={}
for key in sim_word_dict.keys():
        for re_key in sim_word_dict[key]:
                if re_key not in re_dict.keys():
                        re_dict[re_key]=[]
                        if(key not in  re_dict[re_key] and key not in stop_words):
                            re_dict[re_key].append(key)
                else:
                        if(key not in re_dict[re_key] and key not in stop_words):
                                re_dict[re_key].append(key)
with open('./data/reverse_dict/Rever_dict.txt','w') as R_file:
        json.dump(re_dict, R_file, indent=4)

from task_set import task,eps_list,K
tqdm_eps = tqdm.tqdm(eps_list)


for eps in tqdm_eps:
        re_eps_dict={}
        with open(f"./data/p_dict/{args.embedding_type}/{args.mapping_strategy}/eps_{eps}_top_{args.top_k}.txt", 'r') as dic:
                p_dict = json.load(dic)
        with open(f"./data/sim_word_dict/{args.embedding_type}/{args.mapping_strategy}/eps_{eps}_top_{args.top_k}.txt", 'r') as dic:
                sim_word_dict = json.load(dic)
        with open(f"./data/shadow_datasets/{eps}_{task}.txt",'r') as dic:
                S_list=json.load(dic)

        Input_dict={}
        Output_dict={}
        num_all=len(S_list)

        for i in tqdm.trange(num_all):
                Input_tokens=S_list[i][0]
                Output_tokens=S_list[i][1]

                for token in Input_tokens:
                    if token in Input_dict.keys():
                        Input_dict[token]+=1
                    else:
                        Input_dict[token] =1 

        Bayes_score_dict={}
        for key in re_dict.keys():
                candidate_tokens=re_dict[key]
                Bayes_score_list=[]
                Output_Probability=0
                for token in candidate_tokens:
                       if token in Input_dict:
                                Output_Probability+=p_dict[token][sim_word_dict[token].index(key)]*Input_dict[token]
                for token in candidate_tokens:
                        #For each 'Pr(x) + α^(-1)', we multiply it by α. 
                        #The attack result is consistent with the original one.
                        Posterior_Probability=p_dict[token][sim_word_dict[token].index(key)]                  
                        if(token not in Input_dict.keys()):
                                Input_num=1
                               
                        else:
                                Input_num=Input_dict[token]+1
                        Bayes_score_list.append(Posterior_Probability*Input_num)       
                sorted_indices = sorted(range(len(Bayes_score_list)), key=lambda x: Bayes_score_list[x], reverse=True)
                re_eps_dict[key] = [re_dict[key][i] for i in sorted_indices]
                re_eps_dict[key] =re_eps_dict[key][:K]
                sorted_bayes_scores = [Bayes_score_list[i] for i in sorted_indices]
                sorted_bayes_scores=sorted_bayes_scores[:K]
                Bayes_score_dict[key]=sorted_bayes_scores
        with open(f'./data/reverse_dict/re_{eps}_dict.txt','w') as R_file:
                json.dump(re_eps_dict, R_file, indent=4)
        with open(f'./data/Scores/Bayes_score/{eps}_{task}.txt','w') as BS_file:
                json.dump(Bayes_score_dict, BS_file, indent=4)
        

              
              

                        
                      

        
               
               