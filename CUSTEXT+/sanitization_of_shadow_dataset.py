from data.utils import *
from data.args import *
import os
import json
from nltk.tokenize import word_tokenize
import tqdm
import random
random.seed(42)
from task_set import task

parser = get_parser()
args = parser.parse_args()


def Data_loader_sst2(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        next(file)
        all_data=[]  
        for row in file:
                sentence,label=row.strip().split('\t')
                all_data.append(sentence)
        random.shuffle(all_data)
        return all_data[task_num:int(task_num*(shadow_percentage+1))]

def Data_loader_qnli(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        next(file)
        all_data=[]  
        for row in file:
                index,question,sentence,label=row.strip().split('\t')
                all_data.append(sentence)
        random.shuffle(all_data)
        return all_data[task_num:int(task_num*(shadow_percentage+1))]
def Data_loader_agnews(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        all_data=[]  
        for row in file:
                sentence=row.strip()
                all_data.append(sentence)
        random.shuffle(all_data)
        return all_data[task_num:int(task_num*(shadow_percentage+1))]    
shadow_percentage=0.05

task_num=20000
from task_set import task,eps_list


if task=="sst2":
        Data_path="./Dataset/SST-2/train.tsv"
        raw_data=Data_loader_sst2(Data_path)
        for eps in eps_list: 
                parser = get_parser()
                args = parser.parse_args()
                with open(f'./data/shadow_datasets/{eps}_sst2.txt','w') as P_file:
                        with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                                p_dict = json.load(dic)
                        with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                                sim_word_dict = json.load(dic)                
                        json_file=[]
                        for i in tqdm.trange(int(task_num*shadow_percentage)):
                                json_temp=[]
                                data=raw_data[i]
                                words = word_tokenize(data)
                                first_50_words = words
                                json_temp.append(first_50_words)
                                for k in range(100):
                                        new_first_50_words=generate_single_new_sent(old_task = first_50_words ,sim_word_dict = sim_word_dict ,p_dict = p_dict )
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4) 

if task=="qnli":
        Data_path="./Dataset/QNLI/train.tsv"
        raw_data=Data_loader_qnli(Data_path)

        for eps in eps_list: 
                parser = get_parser()
                args = parser.parse_args()
                with open(f'./data/shadow_datasets/{eps}_qnli.txt','w') as P_file:
                        with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                                p_dict = json.load(dic)
                        with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                                sim_word_dict = json.load(dic)                
                        json_file=[]
                        for i in tqdm.trange(int(task_num*shadow_percentage)):
                                json_temp=[]
                                data=raw_data[i]
                                words = word_tokenize(data)
                                first_50_words = words
                                json_temp.append(first_50_words)
                                for k in range(100):
                                        new_first_50_words=generate_single_new_sent(old_task = first_50_words ,sim_word_dict = sim_word_dict ,p_dict = p_dict )
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4) 
                                                                
if task=="agnews":
        Data_path="./Dataset/AGNEWS/train.tsv"
        raw_data=Data_loader_agnews(Data_path)

        for eps in eps_list: 
                parser = get_parser()
                args = parser.parse_args()
                with open(f'./data/shadow_datasets/{eps}_agnews.txt','w') as P_file:
                        with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                                p_dict = json.load(dic)
                        with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                                sim_word_dict = json.load(dic)                
                        json_file=[]
                        for i in tqdm.trange(int(task_num*shadow_percentage)):
                                json_temp=[]
                                data=raw_data[i]
                                words = word_tokenize(data)
                                first_50_words = words
                                json_temp.append(first_50_words)
                                for k in range(100):
                                        new_first_50_words=generate_single_new_sent(old_task = first_50_words ,sim_word_dict = sim_word_dict ,p_dict = p_dict )
                                        json_temp.append(new_first_50_words)
                                json_file.append(json_temp)

                        json.dump(json_file, P_file, indent=4) 

print(f"\n=====Sanitized sentences of the shadow dataset on {task} are already in folders.=====\n")



