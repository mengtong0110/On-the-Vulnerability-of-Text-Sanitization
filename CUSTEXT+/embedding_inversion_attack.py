import torch
import torch.nn as nn
from nltk.corpus import stopwords
from torch.optim import Adam, lr_scheduler
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, BertTokenizer
import os
import json
import torch.nn.functional as F
from tqdm import tqdm
import random
from data.utils import *
random.seed(42)

class BertSST2Model(nn.Module):
    def __init__(self, class_size, pretrained_name='bert-base-uncased'):
        super(BertSST2Model, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_name, return_dict=True)
        self.classifier = nn.Linear(768, class_size)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None):
        output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
        return self.classifier(output.pooler_output)


def load_categories(data_path1):
    categories_san = set() 
    with open(data_path1, 'r', encoding="utf8") as file:
        data_=json.load(file)
        for sample in data_:
            for i in range(len(sample[0])):
                if(sample[0][i] not in stop_words and sample[0][i] in p_dict and sample[1][i] in p_dict and sample[1][i] in R_dict):
                    categories_san.add(sample[1][i])
    return categories_san

from task_set import task,eps_list

def load_sentence(data_path1,data_path2, categories_s):

    test_data = []
    train_data = []
    with open(data_path1, 'r', encoding="utf8") as file:
            data_=json.load(file)
    for san_token in  tqdm(categories_s,desc=f"{task} is loading"):           
        
            for sample in data_:
                for i in range(len(sample[0])):
                    if sample[1][i]==san_token and sample[0][i] not in stop_words and sample[0][i] in p_dict and sample[1][i] in R_dict:
                        BY_c=[]
                        BY_t2=[]
                        BY_t1=[]
                        if(R_dict[sample[1][i]][0]==sample[0][i]):
                            label=1
                        else:
                            label=0
                        for j in range(len(sample[0])):
                            if len(sample[1][j])>=10:
                                continue
                            if(j<i):
                                BY_c.append(sample[1][j])
                                BY_t1.append(sample[1][j])
                            
                            elif(j>i):
                                BY_c.append(sample[1][j])
                                BY_t2.append(sample[1][j])
                            else:
                                BY_c.append(sample[1][j])

                        BY_c=" ".join(BY_c)
                        BY_t1=" ".join(BY_t1)
                        BY_t2=" ".join(BY_t2)
                        test_data.append([BY_c,BY_t1,BY_t2,sample[0][i],sample[1][i]])


        
            
    random.shuffle(test_data)
    test_data=test_data[:3000]
    return train_data, test_data

class BertDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):

        question, sentence, label = self.dataset[index]
        return question, sentence, label


stop_words = set(stopwords.words('english'))




if task=="sst2":
    for eps in eps_list:
        with open(f'./data/attack_result/EMB_attack/{eps}_sst2.txt','w') as R_file:
            with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                sim_word_dict = json.load(dic)
            with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                p_dict = json.load(dic)
            total_correct_predictions = 0
            total_evaluated_samples = 0
            data_path1 = f"./data/private_datasets/{eps}_sst2.txt"  
            data_path2 = f"./data/shadow_datasets/{eps}_sst2.txt"
            with open(f"./data/reverse_dict/re_{eps}_dict.txt",'r') as Re_file:
                    R_dict=json.load(Re_file)
            with open(f"./data/Scores/Bayes_score/{eps}_sst2.txt",'r') as Rep_file:
                    RP_dict=json.load(Rep_file)
            with open('tokens_ctg.txt','r') as file:
                categories_s=json.load(file)
            train_data, test_data= load_sentence(data_path1,data_path2,categories_s)
            print('test_num',len(test_data))
            for sample in tqdm(test_data,desc="Testing"):           
                raw_token=sample[3]
                attack_result= KNN_ATTACK(sample[4],sim_word_dict,p_dict,1,p_dict.keys())
                if(raw_token in attack_result):
                    total_correct_predictions += 1
                total_evaluated_samples += 1
            print(f"EPS_{eps}_Validation Accuracy: {total_correct_predictions/total_evaluated_samples:.4f}")
            R_file.write(f'{eps} EMB-ATTACK success rate : {total_correct_predictions/total_evaluated_samples:.4f}')


if task=='qnli':
    for eps in eps_list:
        with open(f'./data/attack_result/EMB_attack/{eps}_qnli.txt','w') as R_file:
            with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                sim_word_dict = json.load(dic)
            with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                p_dict = json.load(dic)
            total_correct_predictions = 0
            total_evaluated_samples = 0
            data_path1 = f"./data/private_datasets/{eps}_qnli.txt" 
            data_path2 = f"./data/shadow_datasets/{eps}_qnli.txt"
            with open(f"./data/reverse_dict/re_{eps}_dict.txt",'r') as Re_file:
                    R_dict=json.load(Re_file)
            with open(f"./data/Scores/Bayes_score/{eps}_qnli.txt",'r') as Rep_file:
                    RP_dict=json.load(Rep_file)
            with open('tokens_ctg.txt','r') as file:
                categories_s=json.load(file)
            train_data, test_data= load_sentence(data_path1,data_path2,categories_s)
            print('test_num',len(test_data))
            for sample in tqdm(test_data,desc="Testing"):           
                raw_token=sample[3]
                attack_result= KNN_ATTACK(sample[4],sim_word_dict,p_dict,1,p_dict.keys())
                if(raw_token in attack_result):
                    total_correct_predictions += 1
                total_evaluated_samples += 1
            print(f"EPS_{eps}_Validation Accuracy: {total_correct_predictions/total_evaluated_samples:.4f}")
            R_file.write(f'{eps} EMB-ATTACK success rate : {total_correct_predictions/total_evaluated_samples:.4f}')


            
if task=='agnews':
    for eps in eps_list:
        with open(f'./data/attack_result/EMB_attack/{eps}_agnews.txt','w') as R_file:
            with open(f"./data/sim_word_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt", 'r') as dic:
                sim_word_dict = json.load(dic)
            with open(f'./data/p_dict/glove_840B-300d_json/aggressive/eps_{eps}_top_20.txt','r') as dic:
                p_dict = json.load(dic)
            total_correct_predictions = 0
            total_evaluated_samples = 0
            data_path1 = f"./data/private_datasets/{eps}_agnews.txt"  
            data_path2 = f"./data/shadow_datasets/{eps}_agnews.txt"
            with open(f"./data/reverse_dict/re_{eps}_dict.txt",'r') as Re_file:
                    R_dict=json.load(Re_file)
            with open(f"./data/Scores/Bayes_score/{eps}_agnews.txt",'r') as Rep_file:
                    RP_dict=json.load(Rep_file)
            with open('tokens_ctg.txt','r') as file:
                categories_s=json.load(file)
            train_data, test_data= load_sentence(data_path1,data_path2,categories_s)
            print('test_num',len(test_data))
            for sample in tqdm(test_data,desc="Testing"):           
                raw_token=sample[3]
                attack_result= KNN_ATTACK(sample[4],sim_word_dict,p_dict,1,p_dict.keys())
                if(raw_token in attack_result):
                    total_correct_predictions += 1
                total_evaluated_samples += 1
            print(f"ASR of Embedding Inversion Attack with eps={eps} on {task} is: {total_correct_predictions/total_evaluated_samples:.4f}")
            R_file.write(f'ASR of Embedding Inversion Attack with eps={eps} on {task} is: {total_correct_predictions/total_evaluated_samples:.4f}')